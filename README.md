

2019 最新人工智能课程
===========================

2019年最新人工智能课程（包括Stanford, Berkeley, MIT, CMU 等Top CS Ranking学校 ) , 收集这些课程的目的是为了更好学习“state of the art” 教学成果，而个学习时候最好结合相关作业进行理解和实践。


# NLP/NLU

|#|**Course**|**University**|**Course Description**|**Time**|
|---|----|-----|-----|
|11-741|[Machine Learning and Text Mining](http://nyc.lti.cs.cmu.edu/classes/11-741/s19/index.html)|CMU|This is a full-semester lecture-oriented course (12 units) for the PhD-level, MS-level and undergraduate students who meet the prerequisites. It offers a blend of core theory, algorithms, evaluation methodologies and applications of scalable data analytic techniques. Specifically, the covered topics include Link Analysis,Collaborative Filtering,Social-media Analysis,Web-scale Text Classification,Learning to Rank for Information Retrieval,Deep Learning for Text Analysis,Matrix factorization (with SVD, non-negative and probabilistic matrix completion),Stochastic gradient descent,Statistical significance tests |2019 Spring|
|11-747| [Neural networks for NLP](http://www.phontron.com/class/nn4nlp2019/)|CMU|Neural networks provide powerful new tools for modeling language, and have been used both to improve the state-of-the-art in a number of tasks and to tackle new problems that were not easy in the past. This class will start with a brief overview of neural networks, then spend the majority of the class demonstrating how to apply neural networks to natural language problems. Each section will introduce a particular problem or phenomenon in natural language, describe why it is difficult to model, and demonstrate several models that were designed to tackle this problem. In the process of doing so, the class will cover different techniques that are useful in creating neural network models, including handling variably sized and structured sentences, efficient handling of large data, semi-supervised and unsupervised learning, structured prediction, and multilingual modeling|
|11-731 |[Machine Translation and Sequence-to-Sequence Models](http://www.phontron.com/class/mtandseq2seq2019/#)|CMU|Neural networks provide powerful new tools for modeling language, and have been used both to improve the state-of-the-art in a number of tasks and to tackle new problems that were not easy in the past. This class will start with a brief overview of neural networks, then spend the majority of the class demonstrating how to apply neural networks to natural language problems. Each section will introduce a particular problem or phenomenon in natural language, describe why it is difficult to model, and demonstrate several models that were designed to tackle this problem. In the process of doing so, the class will cover different techniques that are useful in creating neural network models, including handling variably sized and structured sentences, efficient handling of large data, semi-supervised and unsupervised learning, structured prediction, and multilingual modeling.|
|11-468| [Speech Processing Fall 2019](http://tts.speech.cs.cmu.edu/courses/11492/)|CMU|Speech Processing offers a practical and theoretical understanding of how human speech can be processed by computers. It covers speech recognition, speech synthesis and spoken dialog systems. The course involves practicals where the student will build working speech recognition systems, build their own synthetic voice and build a complete spoken dialog system. This work will be based on existing toolkits. Details of algorithms, techniques and limitations of state of the art speech systems will also be presented. This course is designed for students wishing understand how to process real data for real applications, applying statistical and machine learning techniques as well as working with limitations in the technology.|
|17-728| [Machine Learning and Sensing](http://fall19.mayankgoel.courses/)|CMU|Machine learning and sensors are at the core of most modern computing devices and technology. From Amazon Echo to Apple Watch to Google Photos to self-driving cars, making sense of the data coming from powerful but noisy sensors is a critical challenge. The course will aim to explore this intersection of sensors and machine learning, understand the inner workings of modern computing technologies, and design the future ones. We will cover data collection, signal processing, data processing, data visualization, feature engineering, machine learning tools, and some prototyping technologies. The course will focus on class discussions, hands-on demonstrations, and tutorials. We will evaluate students on their class participation, multiple mini-projects, and a final team project.|
|CS224n| [Natural Language Processing with Deep Learning](http://web.stanford.edu/class/cs224n/)|Stanford|Natural language processing (NLP) is one of the most important technologies of the information age, and a crucial part of artificial intelligence. Applications of NLP are everywhere because people communicate almost everything in language: web search, advertising, emails, customer service, language translation, virtual agents, medical reports, etc. In recent years, Deep Learning approaches have obtained very high performance across many different NLP tasks, using single end-to-end neural models that do not require traditional, task-specific feature engineering. In this course, students will gain a thorough introduction to cutting-edge research in Deep Learning for NLP. Through lectures, assignments and a final project, students will learn the necessary skills to design, implement, and understand their own neural network models. This year, CS224n will be taught for the first time using PyTorch rather than TensorFlow (as in previous years).|
|CS11711| [Algorithms for NLP](http://demo.clab.cs.cmu.edu/11711fa18/)|CMU|This course will explore current statistical techniques for the automatic analysis of natural (human) language data. The dominant modeling paradigm is corpus-driven statistical learning, with a split focus between supervised and unsupervised methods. This term we are making Algorithms for NLP a lab-based course. Instead of homeworks and exams, you will complete four hands-on coding projects. This course assumes a good background in basic probability and a strong ability to program in Java. Prior experience with linguistics or natural languages is helpful, but not required. There will be a lot of statistics, algorithms, and coding in this class.Slides, materials, and projects for this iteration of Algorithms for NLP are borrowed from Dan Jurafsky at Stanford, Dan Klein and David Bamman at UC Berkeley and Nathan Schneider at Georgetown|
|CS447| [Natural Language Processing](https://courses.engr.illinois.edu/cs447/fa2019/)|UIUC|This course provides an introduction to computational linguistics, from morphology (word formation) and syntax (sentence structure) to semantics (meaning), and natural language processing applications such as parsing, machine translation, generation and dialog systems.|
|CS4300| [Language and Information, Spring 2019](http://www.cs.cornell.edu/courses/cs4300/2019sp/)|Cornell|How to make sense of the vast amounts of information available online, and how to relate it and to the social context in which it appears? This course introduces basic tools for retrieving and analyzing unstructured textual infordia. Applications include information retrieval (with human feedback), sentiment analysis and social analysis of text. The coursework will include programming projects that play on the interaction between knowledge and social factors.|
|CS5740| [Natural Language Processing (Spring 2019)](http://www.cs.cornell.edu/courses/cs5740/2019sp/)|Cornell|This course constitutes a depth-first technical introduction to natural language processing (NLP). NLP is at the heart of many of today’s most exciting technological achievements, including machine translation, automatic conversational assistants and Internet search. The goal of the course is to provide a deep understanding of the language of the field, including algorithms, core problems, methods, and data. Possible topics include text classification, lexical semantics, language modeling, machine translation, tagging and sequence modeling, parsing, compositional semantics, summarization, question answering, language grounding, information extraction, and sentiment analysis.|
|COS484|[Natural Langauge Processing](https://nlp.cs.princeton.edu/cos484/)|Princeton|Recent advances have ushered in exciting developments in natural language processing (NLP), resulting in systems that can translate text, answer questions and even hold spoken conversations with us. This course will introduce students to the basics of NLP, covering standard frameworks for dealing with natural language as well as algorithms and techniques to solve various NLP problems, including recent deep learning approaches. Topics covered include language modeling, representation learning, text classification, sequence tagging, syntactic parsing, machine translation, question answering and others.|
|CS6788|[Advanced Topic Modeling](https://mimno.infosci.cornell.edu/info6150/)|Cornell|Topic models, and other unsupervised matrix factorization methods, have emerged as a powerful technique in machine learning and data science because they sit at an ideal point between simplicity and complexity. Data analysts like them because they provide sophisticated insight without attempting real natural language understanding. From an algorithmic standpoint, they are an excellent test case for new inference methods because the topic model objective is hard in the sense that it is multi-modal and non-convex, but it is also reasonably easy to find good approximate solutions.|
|COMSW4705| [Natural Language Processing](http://www.cs.columbia.edu/~mcollins/cs4705-spring2019/)|Columbia| COMS W4705 is a graduate introduction to natural language processing, the study of human language from a computational perspective. We will cover syntactic, semantic and discourse processing models. The emphasis will be on machine learning or corpus-based methods and algorithms. We will describe the use of these methods and models in applications including syntactic parsing, information extraction, statistical machine translation, dialogue systems, and summarization.|
|L101|[Machine Learning for Language Processing](https://www.cl.cam.ac.uk/teaching/1819/L101/)|Cambridge|This module aims to provide an introduction to machine learning with specific application to tasks such as document classification, spam email filtering, language modelling, part-of-speech tagging, and named entity and event recognition for textual information extraction. We will cover supervised, weakly-supervised and unsupervised approaches using generative and discriminative classifiers based on graphical models, including (hidden) Markov models and CRFs, and clustering / dimensionality-reduction methods, such as latent Dirichlet allocation and neural word embeddings.|
|L95|[Introduction to Natural Language Syntax and Parsing](https://www.cl.cam.ac.uk/teaching/1819/L95/)|Cambridge|This module aims to provide a brief introduction to linguistics for computer scientists and then goes on to cover some of the core tasks in natural language processing (NLP), focussing on statistical parsing of sentences to yield syntactic and semantic representations. We will look at how to evaluate parsers and see how well state-of-the-art tools perform given current techniques.|
|L90|[Overview of Natural Language Processing](https://www.cl.cam.ac.uk/teaching/1819/L90/)|Cambridge|This course introduces the fundamental techniques of natural language processing. It aims to explain the potential and the main limitations of these techniques. Some current research issues are introduced and some current and potential applications discussed and evaluated. Students will also be introduced to practical experimentation in natural language processing.|
||[Natural Language Processing](https://www.cl.cam.ac.uk/teaching/1819/NLP/)|Cambridge|This course introduces the fundamental techniques of natural language processing. It aims to explain the potential and the main limitations of these techniques. Some current research issues are introduced and some current and potential applications discussed and evaluated. Students will also be introduced to practical experimentation in natural language processing.|
||[Denotational Semantics](https://www.cl.cam.ac.uk/teaching/1819/DenotSem/)|Cambridge|The aims of this course are to introduce domain theory and denotational semantics, and to show how they provide a mathematical basis for reasoning about the behaviour of programming languages.|
|CS431|[Introduction to Natural Language Processing](https://coling.epfl.ch/)|EPFL|The objective of this course is to present the main models, formalisms and algorithms necessary for the development of applications in the field of natural language information processing. The concepts introduced during the lectures will be applied during practical sessions.|
|CS249|[Advanced Seminar: Learning from Text](http://oak.cs.ucla.edu/classes/cs249/)|UCLA|Humans use language to perceive, describe, communicate, and record the world around them, so text is the primary “data” generated by humans. Despite the importance of text data written in natural languages, making computers “understand” them is still a challenging task. In this class, we will go through key papers in natural language processing (NLP) throughout the quarter and learn (1) how NLP problems can be formulated as a computational problem and (2) the basic algorithms that solve the formulated problems.|
|CS269|[Seminar: Machine Learning in Natural Language Processing](https://uclanlp.github.io/CS269-17/overview)|UCLA|Natural language processing (NLP) enables computers to use and understand human languages. Recently, NLP techniques have been widely used in many applications including machine translation, question answering, and extracting information from text. In this course, we will cover the fundamental elements and recent research trends in NLP. Tentative topics include syntactic analysis, semantic analysis, and NLP applications as well as the underlying machine learning methods that widely used in modeling NLP systems. The activities of the course include lectures, paper presentations, quizzes, a critical review report, and a final project.|
|COSIW06|[Natural Language Generation](https://sites.google.com/view/nlg-spring19)|Princeton|Recent advances in deep learning have led to exciting developments in natural language processing, especially in areas like translation, question answering and sentiment analysis. However, generating accurate and fluent language remains a major challenge even today. This seminar will study the latest research in natural language generation and allow students to choose and work on a research project in this space. The project can focus on either new theoretical or algorithmic developments in language generation, applications of generation to tasks in NLP, or development of new evaluation metrics for generation systems.  |
|CSC401|[Natural Language Computing](http://www.cs.toronto.edu/~frank/csc401/)|Toronto|This course presents an introduction to natural language computing in applications such as information retrieval and extraction, intelligent web searching, speech recognition, and machine translation. These applications will involve various statistical and machine learning techniques. Assignments will be completed in Python. All code must run on the 'teaching servers'.|
|CSC485/2501| [Introduction to Computational Linguistics](http://www.cs.toronto.edu/~gpenn/csc485/index.html)|Toronto|Introduction to Computational Linguistics Syntax Parsing Ambiguity Resolution Statistical Attachment Disambiguation Lexical Semantics Word Sense Disambiguation Statistical Parsing Anaphora Semantic Representation and Processing|
|CSC2518| [Spoken Language Processing](http://www.cs.toronto.edu/~gpenn/csc2518/index.html)|Toronto|Robustness/Learned Representations Reductive Architectures/Under-resourced Languages Intelligibility/Enhancement/Adaptation Policy Gradients/Reinforcement Learning Time-domain/Phase Representations|
|CS4705| [Introduction to Natural Language Processing]( http://www.cs.columbia.edu/~kathy/NLP/2019/)|columbia|This course provides an introduction to the field of natural language processing (NLP). We will learn how to create systems that can analyze, understand and produce language. We will begin by discussing machine learning methods for NLP as well as core NLP, such as language modeling, part of speech tagging and parsing. We will also discuss applications such as information extraction, machine translation, text generation and automatic summarization. The course will primarily cover statistical and machine learning based approaches to language processing, but it will also introduce the use of linguistic concepts that play a role. We will study machine learning methods currently used in NLP, including supervised machine learning, hidden markov models, and neural networks. Homework assignments will include both written components and programming assignments.|
|CSE517|  [Natural Language Processing](https://courses.cs.washington.edu/courses/cse517/19wi/)|washington| Introduction to language models, HMM/EM/PCFG/CRFS/NN|
||[TEXT MINING](https://mimno.infosci.cornell.edu/info3350/)|cornell|The course will introduce students to research methods in computer-assisted scholarship. We will learn to represent text documents in computational forms, and to appreciate the effect of choices we make in this process. We will cover a selection of popular tools such as classification, clustering, and topic modeling. Each week we will discuss both the details of computational methods and how each method can be applied in the context of scholarly research.|
|INFO/CS4300| [Language and Information](http://www.cs.cornell.edu/courses/cs4300/2019sp/index.html)|cornell|How to make sense of the vast amounts of information available online, and how to relate it and to the social context in which it appears? This course introduces basic tools for retrieving and analyzing unstructured textual infordia. Applications include information retrieval (with human feedback), sentiment analysis and social analysis of text. The coursework will include programming projects that play on the interaction between knowledge and social factors.|
|CS4740/5740| [Introduction to Natural Language Processing](https://www.cs.cornell.edu/courses/cs4740/2019fa/)|cornell|This course constitutes an introduction to natural language processing (NLP), the goal of which is to enable computers to use human languages as input, output, or both. NLP is at the heart of many of today's most exciting technological achievements, including machine translation, automatic conversational assistants and Internet search. Possible topics include methods for handling underlying linguistic phenomena (e.g., syntactic analysis, word sense disambiguation and discourse analysis) and vital emerging applications (e.g., machine translation, sentiment analysis, summarization and information extraction).|
|TCD2018|[Quantitative Text Analysis](https://kenbenoit.net/quantitative-text-analysis-tcd-2018/)|LSE|The course surveys methods for systematically extracting quantitative information from political text for social scientific purposes, starting with classical content analysis and dictionary-based methods, to classification methods, and state-of-the-art scaling methods and topic models for estimating quantities from text using statistical techniques. The course lays a theoretical foundation for text analysis but mainly takes a very practical and applied approach, so that students learn how to apply these methods in actual research. The common focus across all methods is that they can be reduced to a three-step process: first, identifying texts and units of texts for analysis; second, extracting from the texts quantitatively measured features - such as coded content categories, word counts, word types, dictionary counts, or parts of speech - and converting these into a quantitative matrix; and third, using quantitative or statistical methods to analyse this matrix in order to generate inferences about the texts or their authors. The course systematically covers these methods in a logical progression, with a practical, hands-on approach where each technique will be applied using appropriate software to real texts.|
|CS6740|[Advanced Language Technologies](https://www.cs.cornell.edu/courses/cs6740/2019fa/)|cornell|This course covers selected advanced topics in natural language processing (NLP) and/or information retrieval, with a conscious attempt to avoid topics covered by other Cornell courses.|
|CS5740| [Natural Language Processing ](https://www.cs.cornell.edu/courses/cs5740/2019sp/)|cornell|This course constitutes a depth-first technical introduction to natural language processing (NLP). NLP is at the heart of many of today’s most exciting technological achievements, including machine translation, automatic conversational assistants and Internet search. The goal of the course is to provide a deep understanding of the language of the field, including algorithms, core problems, methods, and data. Possible topics include text classification, lexical semantics, language modeling, machine translation, tagging and sequence modeling, parsing, compositional semantics, summarization, question answering, language grounding, information extraction, and sentiment analysis.|
|CS6742|[Natural Language Processing and Social Interaction](https://www.cs.cornell.edu/courses/cs6742/2019fa/)|cornell||
||[introduction to audio content analysis](https://www.audiocontentanalysis.org/teaching/)|Georgia Tech|This course is an introduction to the software-based analysis of digital music signals (Music Information Retrieval) for students with existing background in audio processing. It covers the basic approaches for audio content analysis and provides students with the necessary algorithmic background to approach this class of problems. Topics include, for example, pitch tracking, beat tracking, audio feature extraction, and genre classification.|
|CSE8803|[Deep Learning for Text Data](http://chaozhang.org/course/19f-cse8803.html)|Georgia Tech|This course will introduce state-of-the-art machine learning techniques for mainstay problems in text data analysis, with particular emphasis on deep learning methods that have recently achieved enormous success. The students will learn about trending problems in this field, key methods for solving these problems, and their advantages and disadvantages. The students are also expected to read and present cutting edge research papers, as well as conduct a research oriented course project. The course will provide useful techniques for students who want to solve practical problems involving text data, and better prepare those who want to do edge-cutting research in text mining, information retrieval, natural language processing, and text-rich interdisciplinary research. The learning objective is that by the end of this course, the students are able to formulate their text analysis problems at hand, choose appropriate statistical models for the problems, and even come up with innovative solutions for solving open research problems in this field.|
|CSE447/547M|[ Natural Language Processing](https://courses.cs.washington.edu/courses/cse447/19wi/)|washington|Natural language processing (NLP) seeks to endow computers with the ability to intelligently process human language. NLP components are used in conversational agents and other systems that engage in dialogue with humans, automatic translation between human languages, automatic answering of questions using large text collections, the extraction of structured information from text, tools that help human authors, and many, many more. This course will teach you the fundamental ideas used in key NLP components.|
|CSE599D1|: [Advanced Topics in Natural Language Processing ](https://wammar.github.io/2018sp_uw_cse_599/)|Washington|In this course, we will review some of the highly influential papers which had a sustained impact on NLP research. Each week, participants will either give a 10-20 minute presentation or write a peer review for one of the focus papers for that week. Paper presentations will be followed by a group discussion of what influence this paper had in NLP. The papers will cover a mixture of topics including information extraction, syntactic parsing, semantic parsing, structured models, reinforcement learning and dataset construction.|
|CS4650/7650|[Natural Language](https://github.com/jacobeisenstein/gt-nlp-class)|Georgia Tech|This course gives an overview of modern data-driven techniques for natural language processing. The course moves from shallow bag-of-words models to richer structural representations of how words interact to create meaning. At each level, we will discuss the salient linguistic phemonena and most successful computational models. Along the way we will cover machine learning techniques which are especially relevant to natural language processing.|
|DS-GA1012|[Natural Language Understanding and Computational Semantics ](https://docs.google.com/document/d/1kXhxA4iit2fhAJJGOb32bb151cKLJtW8xWuyMVwqD6s/edit)|New York University|Since at least the proposal of the Turing test, building computational systems that can communicate with humans using natural language has been a central goal for what we now think of as AI research. Understanding real, naturally occurring human language is the key to reaching this goal. This course surveys recent successes in language understanding, but it is focused primarily on preparing students to do original research in this area, culminating with a substantial final project.The course is centered on text rather than speech, but within that, it will touch on the full range of applicable techniques for language understanding, including formal logics, statistical methods, distributional methods, and deep learning, and will bring in ideas from formal linguistics where they can be readily used in practice. We’ll discuss tasks like sentiment analysis, word similarity, and question answering, as well as higher level issues like how to effectively represent language meaning.|
|LING-UA52|[Machine Learning for Language Understanding]( https://docs.google.com/document/d/1EsKTFDOJ0DksxkC5tnCNjkqCM_3UB1Q2MbYY4OHQkB0/edit#heading=h.ve7jgh6fmm2k)|New York University|Building computational models that can understand human language has long been a goal for researchers in computational linguistics and in the area of artificial intelligence called natural language processing. Many of the biggest successes in research toward this goal have relied on machine learning: a family of methods that allow computers to learn to reproduce some human behavior by example, rather than by explicit programming. This course covers widely-used machine learning methods for language understanding—with a special focus on machine learning methods based on artificial neural networks—and culminates in a substantial final project in which students write an original research paper in AI or computational linguistics.|
|601.765|[ Machine Learning: Linguistic & Sequence Modeling](https://seq2class.github.io/)|Johns Hopkins University|This course surveys formal ingredients that are used to build structured models of character and word sequences. We will unpack recent deep learning architectures that consider various kinds of latent structure, and see how they draw on earlier work in structured prediction, dimensionality reduction, Bayesian nonparametrics, multi-task learning, etc. We will also examine a range of strategies used for inference and learning in these models. Students will be expected to read recent papers and carry out a research project|
|CS395T| [Grounded Natural Language Processing](http://www.cs.utexas.edu/~mooney/gnlp/)|The University of Texas at Austin|This course will be a graduate research seminar in grounded natural language processing (GNLP), a subarea of AI that studies the connection between natural language and perception and action in the world. It makes connections between natural language processing (NLP) and computer vision, robotics, and computer graphics. Almost all work in the area uses machine learning to learn the connection between language and perception and/or action from some form of multi-modal training data.|
|CS378| [Practical Applications of Natural Language Processing ](http://www.cs.utexas.edu/users/porter/syllabus-378-Spring19.php)|The University of Texas at Austin|Automatically extracting information from natural-language text is one of the great scientific challenges in AI, and it also offers significant practical and commercial benefits. This class will explore the state of the art in applications of Natural Language Processing (NLP) through a series of increasingly ambitious projects. Each project is inspired by real use cases, sometimes with datasets provided by local companies. For each project, we will read research publications and investigate algorithms and tools that might apply.|
|LIN389C| [Research in Computational Linguistics](http://www.katrinerk.com/courses/lin-389c-research-in-computational-linguistics)|The University of Texas at Austin|This is a research course for students who work in computational linguistics. In the course, we discuss current research by course participants, review foundational knowledge that is relevant to participants' research, and talk about big-picture issues and current research in field.|
|LIN353C|[Introduction to computational linguistics](http://www.katrinerk.com/courses/intro-to-computational-linguistics-ug)|The University of Texas at Austin|Text is everywhere, in huge amounts: Books, emails, web pages, scientific papers, and so on. To be able to use the information laid down in all this text, we need technology that can help us manage, understand, sort, and make sense of all the information, for example: Automatically translating texts from one language to another; building better search engines that can deal with complex questions instead of just keywords; figuring out automatically whether the blogs are saying good or bad things about a particular product; extracting useful facts from repositories of scientific papers about medicine.Computational linguistics is about using mathematical and computational methods to better describe how language works. It is about developing algorithms for automatic language understanding. And it is about building language technology applications. As you can see, computational linguistics spans a wide range of questions, from linguistics to computer science. It also draws on other fields such as cognition, and philosophy of language.|
|CS388|[Natural Language Processing ](http://www.cs.utexas.edu/~gdurrett/courses/fa2019/cs388.shtml)|The University of Texas at Austin|This class is a graduate-level introduction to Natural Language Processing (NLP), the study of computing systems that can process, understand, or communicate in human language. The course covers fundamental approaches, largely machine learning and deep learning, used across the field of NLP as well as a comprehensive set of NLP tasks both historical and contemporary. Techniques studied include Bayes nets, conditional random fields, structured max-margin, tree-structured models, recurrent and convolutional neural networks, and attention mechanisms. Problems range from syntax (part-of-speech tagging, parsing) to semantics (lexical semantics, question answering, grounding) and include various applications such as summarization, machine translation, information extraction, and dialogue systems. Programming assignments throughout the semester involve building scalable machine learning systems for various NLP tasks.|
